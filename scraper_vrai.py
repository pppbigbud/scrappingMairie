#!/usr/bin/env python3
"""
SCRAPER VRAI - Plus grandes agglomÃ©rations franÃ§aises
Cible les sites de mairies des prÃ©fectures et sous-prÃ©fectures
"""

import requests
from bs4 import BeautifulSoup
import re
import json
from dataclasses import dataclass, asdict
from typing import List, Optional
import time
from urllib.parse import urljoin, urlparse

@dataclass 
class Opportunite:
    commune: str
    departement: str
    date: str
    titre: str
    contenu: str
    mots_cles: List[str]
    url_source: str
    confiance: str

# ğŸ›ï¸ PLUS GRANDES AGGLOMÃ‰RATIONS FRANÃ‡AISES
# Liste des prÃ©fectures et grandes villes avec leurs URLs de dÃ©libÃ©rations
VILLES_CIBLES = {
    # MÃ©tropoles (> 500 000 hab)
    'Marseille': {
        'dept': '13', 'region': 'Provence-Alpes-CÃ´te d\'Azur',
        'urls': [
            'https://www.marseille.fr/mairie/deliberations-du-conseil-municipal',
        ]
    },
    'Lyon': {
        'dept': '69', 'region': 'Auvergne-RhÃ´ne-Alpes',
        'urls': [
            'https://www.lyon.fr/demarche/deliberations-conseil-municipal',
        ]
    },
    'Toulouse': {
        'dept': '31', 'region': 'Occitanie',
        'urls': [
            'https://www.toulouse.fr/web/decouverte/deliberations',
        ]
    },
    'Nice': {
        'dept': '06', 'region': 'Provence-Alpes-CÃ´te d\'Azur',
        'urls': [
            'http://deliberations.nice.fr/',
        ]
    },
    'Nantes': {
        'dept': '44', 'region': 'Pays de la Loire',
        'urls': [
            'https://www.nantes.fr/home/demarche/deliberations.html',
        ]
    },
    'Strasbourg': {
        'dept': '67', 'region': 'Grand Est',
        'urls': [
            'https://www.strasbourg.eu/deliberations-conseil-municipal',
        ]
    },
    'Montpellier': {
        'dept': '34', 'region': 'Occitanie',
        'urls': [
            'https://www.montpellier.fr/4027-deliberations-du-conseil-municipal.htm',
        ]
    },
    'Bordeaux': {
        'dept': '33', 'region': 'Nouvelle-Aquitaine',
        'urls': [
            'https://www.bordeaux.fr/o43771/deliberations',
        ]
    },
    'Lille': {
        'dept': '59', 'region': 'Hauts-de-France',
        'urls': [
            'https://www.lille.fr/Nos-dossiers/La-vie-communale/Les-deliberations',
        ]
    },
    # Villes moyennes importantes (100 000 - 500 000 hab)
    'Rennes': {
        'dept': '35', 'region': 'Bretagne',
        'urls': [
            'https://metropole.rennes.fr/les-deliberations',
        ]
    },
    'Reims': {
        'dept': '51', 'region': 'Grand Est',
        'urls': [
            'https://www.reims.fr/municipalite/les-elus/conseil-municipal/les-deliberations',
        ]
    },
    'Toulon': {
        'dept': '83', 'region': 'Provence-Alpes-CÃ´te d\'Azur',
        'urls': [
            'https://www.toulon.fr/decouvrir-la-ville/mairie-deliberations.html',
        ]
    },
    'Grenoble': {
        'dept': '38', 'region': 'Auvergne-RhÃ´ne-Alpes',
        'urls': [
            'https://www.grenoble.fr/deliberations-du-conseil-municipal-de-grenoble',
        ]
    },
    'Dijon': {
        'dept': '21', 'region': 'Bourgogne-Franche-ComtÃ©',
        'urls': [
            'https://www.dijon.fr/conseil-municipal-et-comite-metropolitain/conseil-municipal/les-deliberations',
        ]
    },
    'Angers': {
        'dept': '49', 'region': 'Pays de la Loire',
        'urls': [
            'https://www.angers.fr/lactu-municipale/les-deliberations/index.html',
        ]
    },
}

MOTS_CLES = [
    'chaufferie', 'biomasse', 'chaudiÃ¨re bois', 'chaudiÃ¨re biomasse',
    'bois Ã©nergie', 'rÃ©seau chaleur', 'chaleur renouvelable',
    'chaufferie collective', 'chauffage bois', 'granulÃ©s',
    'plaquettes forestiÃ¨res', 'chaudiÃ¨re granulÃ©s'
]

class ScraperVrai:
    """Vrai scraping des sites de mairies"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.opportunites = []
        
    def scraper_ville(self, ville: str, config: dict) -> List[Opportunite]:
        """
        Scraping d'une ville - tente toutes les URLs fournies
        """
        print(f"\nğŸ™ï¸ [{ville}] ({config['dept']})")
        print("-" * 60)
        
        opportunites = []
        
        for url in config['urls']:
            try:
                print(f"ğŸ” {url}")
                resp = self.session.get(url, timeout=20, allow_redirects=True)
                
                if resp.status_code == 200:
                    opps = self._analyser_page(resp.text, ville, config, url)
                    if opps:
                        opportunites.extend(opps)
                        print(f"   âœ… {len(opps)} opportunitÃ©(s)")
                    else:
                        print(f"   âš ï¸ Aucune opportunitÃ© trouvÃ©e")
                else:
                    print(f"   âŒ Erreur {resp.status_code}")
                    
            except requests.exceptions.Timeout:
                print(f"   â±ï¸ Timeout")
            except Exception as e:
                print(f"   ğŸ’¥ Erreur: {str(e)[:50]}")
            
            time.sleep(1)  # Respecter les serveurs
        
        return opportunites
    
    def _analyser_page(self, html: str, ville: str, config: dict, url_source: str) -> List[Opportunite]:
        """Analyse une page HTML pour trouver les dÃ©libÃ©rations"""
        opportunites = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # Chercher tous les liens qui pourraient Ãªtre des dÃ©libÃ©rations
        # Pattern 1: Liens avec "deliberation", "conseil", "actes" dans le href
        liens = soup.find_all('a', href=re.compile(r'deliberation|conseil|actes|pv|document', re.I))
        
        # Pattern 2: Divs qui contiennent les termes recherchÃ©s
        # On cherche dans tout le texte de la page
        texte_complet = soup.get_text().lower()
        
        # VÃ©rifier si au moins un mot-clÃ© est prÃ©sent
        mots_trouves_page = []
        for mot in MOTS_CLES:
            if mot.lower() in texte_complet[:50000]:  # Limite Ã  50k caractÃ¨res
                mots_trouves_page.append(mot)
        
        if mots_trouves_page:
            print(f"   ğŸ”‘ Mots-clÃ©s trouvÃ©s sur la page: {', '.join(mots_trouves_page[:3])}")
        
        # Pour chaque lien, vÃ©rifier s'il contient des mots-clÃ©s
        for lien in liens[:20]:  # Limiter Ã  20 liens pour le POC
            try:
                titre = lien.get_text(strip=True)
                href = lien.get('href', '')
                
                if not titre or len(titre) < 5:
                    continue
                
                # Chercher les mots-clÃ©s dans le titre
                titre_lower = titre.lower()
                mots_trouves = []
                
                for mot in MOTS_CLES:
                    if mot.lower() in titre_lower:
                        nb_mots = len(titre_lower.split())
                        if nb_mots < 100:  # VÃ©rifier que c'est un titre court
                            mots_trouves.append(mot)
                
                if mots_trouves:
                    # Construire URL complÃ¨te
                    if href.startswith('http'):
                        url_complete = href
                    else:
                        url_complete = urljoin(url_source, href)
                    
                    # DÃ©terminer confiance
                    confiance = 'forte' if len(mots_trouves) >= 2 else 'moyenne'
                    
                    opp = Opportunite(
                        commune=ville,
                        departement=f"{config['dept']} - {config['region']}",
                        date="Date non extraite",
                        titre=titre[:150],
                        contenu=f"Projet dÃ©tectÃ© via scraping: {titre}",
                        mots_cles=mots_trouves[:5],
                        url_source=url_complete,
                        confiance=confiance
                    )
                    opportunites.append(opp)
                    
            except Exception:
                continue
        
        return opportunites
    
    def lancer_veille_nationale(self, max_villes: int = 10) -> List[Opportunite]:
        """
        Lance la veille sur les X plus grandes villes
        """
        print("=" * 70)
        print("ğŸ”¥ VRAI SCRAPING - PLUS GRANDES AGGLOMÃ‰RATIONS")
        print("=" * 70)
        print(f"ğŸ¯ {min(max_villes, len(VILLES_CIBLES))} villes Ã  analyser")
        print("ğŸ›ï¸ Cible: PrÃ©fectures et grandes villes")
        print("â±ï¸ Temps estimÃ©: 2-3 minutes")
        print()
        
        toutes_opps = []
        villes_list = list(VILLES_CIBLES.items())[:max_villes]
        
        for i, (ville, config) in enumerate(villes_list, 1):
            print(f"\n[{i}/{len(villes_list)}] ", end="")
            opps = self.scraper_ville(ville, config)
            toutes_opps.extend(opps)
        
        print("\n" + "=" * 70)
        print(f"ğŸ“Š RÃ‰SULTAT: {len(toutes_opps)} opportunitÃ©s trouvÃ©es")
        print("=" * 70)
        
        if toutes_opps:
            print("\nğŸ¯ TOP RÃ‰SULTATS:")
            for i, opp in enumerate(toutes_opps[:5], 1):
                emoji = "ğŸ”´" if opp.confiance == "forte" else "ğŸŸ "
                print(f"{i}. {emoji} {opp.commune}: {opp.titre[:60]}...")
        
        return toutes_opps


def main():
    """Test CLI"""
    scraper = ScraperVrai()
    resultats = scraper.lancer_veille_nationale(max_villes=15)
    
    if resultats:
        print(f"\nğŸ’¾ {len(resultats)} opportunitÃ©s exportÃ©es")
    else:
        print("\nğŸ¤· Aucune opportunitÃ© trouvÃ©e")
        print("ğŸ’¡ Les sites peuvent bloquer le scraping ou changer de structure")


if __name__ == '__main__':
    main()
