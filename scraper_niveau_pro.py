#!/usr/bin/env python3
"""
SCRAPER NIVEAU PRO - EXTRACTION MASSIVE AUVERGNE-RHÃ”NE-ALPES
Mission: TROUVER DES RÃ‰SULTATS Ã  tout prix pour l'entretien de Frank
StratÃ©gie: Ratissage large + sources alternatives + contournement total
"""

import requests
from bs4 import BeautifulSoup, Comment
import json
import re
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from typing import List, Optional
import time
import random
from urllib.parse import urljoin, urlparse, parse_qs
import base64

@dataclass
class Opportunite:
    commune: str
    departement: str
    region: str
    source: str
    date: str
    titre: str
    description: str
    mots_cles: List[str]
    url_source: str
    confiance: str
    population: Optional[int] = None
    budget_estime: Optional[str] = None
    contact: Optional[str] = None

# MOTS-CLÃ‰S Ã‰LARGIS - StratÃ©gie agressive
MOTS_CLES_PRIORITAIRES = [
    'chaufferie', 'biomasse', 'chaudiÃ¨re bois', 'bois Ã©nergie', 
    'rÃ©seau chaleur', 'chaufferie collective', 'chaudiÃ¨re biomasse',
    'chaleur renouvelable', 'gÃ©othermie', 'pompe Ã  chaleur'
]

MOTS_CLES_SECONDAIRES = [
    'chauffage collectif', 'granulÃ©s', 'plaquettes', 'modernisation chauffage',
    'Ã©nergie renouvelable', 'transition Ã©nergÃ©tique', 'remplacement chaudiÃ¨re',
    'rÃ©novation Ã©nergÃ©tique', 'efficacitÃ© Ã©nergÃ©tique', 'marchÃ© Ã©nergie',
    'appel offre chauffage', 'consultation chauffage', 'travaux chauffage',
    'installation chauffage', 'maintenance chauffage', 'fourniture Ã©nergie'
]

# USER-AGENTS ROTATION - Plus agressifs
USER_AGENTS_PRO = [
    # Chrome Windows
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    # Firefox
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0',
    # Safari Mac
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    # Mobile (pour tromper les filtres)
    'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1',
    'Mozilla/5.0 (Android 14; Mobile; rv:121.0) Gecko/121.0 Firefox/121.0'
]

# SOURCES MASSIVES - Tout ce qui peut contenir des infos
SOURCES_AUVERGNE_RHONE_ALPES = {
    # === SITES MUNICIPAUX Ã‰LARGIS ===
    'municipaux': [
        # Puy-de-DÃ´me (63)
        {'commune': 'Clermont-Ferrand', 'dept': '63', 'url': 'https://www.clermontferrand.fr', 'pop': 147284},
        {'commune': 'ChamaliÃ¨res', 'dept': '63', 'url': 'https://www.chamalieres.fr', 'pop': 17716},
        {'commune': 'Cournon-d\'Auvergne', 'dept': '63', 'url': 'https://www.cournon-auvergne.fr', 'pop': 19627},
        {'commune': 'Riom', 'dept': '63', 'url': 'https://www.ville-riom.fr', 'pop': 18682},
        {'commune': 'Issoire', 'dept': '63', 'url': 'https://www.issoire.fr', 'pop': 13806},
        {'commune': 'Thiers', 'dept': '63', 'url': 'https://www.ville-thiers.fr', 'pop': 11634},
        {'commune': 'AubiÃ¨re', 'dept': '63', 'url': 'https://www.aubiere.fr', 'pop': 10239},
        {'commune': 'Beaumont', 'dept': '63', 'url': 'https://www.beaumont63.fr', 'pop': 11334},
        {'commune': 'Gerzat', 'dept': '63', 'url': 'https://www.gerzat.fr', 'pop': 9865},
        {'commune': 'Ceyrat', 'dept': '63', 'url': 'https://www.ceyrat.fr', 'pop': 6156},
        
        # Allier (03)
        {'commune': 'Vichy', 'dept': '03', 'url': 'https://www.ville-vichy.fr', 'pop': 25789},
        {'commune': 'MontluÃ§on', 'dept': '03', 'url': 'https://www.montlucon.fr', 'pop': 37570},
        {'commune': 'Moulins', 'dept': '03', 'url': 'https://www.moulins.fr', 'pop': 19960},
        {'commune': 'Cusset', 'dept': '03', 'url': 'https://www.cusset.fr', 'pop': 12617},
        {'commune': 'Yzeure', 'dept': '03', 'url': 'https://www.yzeure.fr', 'pop': 12760},
        
        # Cantal (15)
        {'commune': 'Aurillac', 'dept': '15', 'url': 'https://www.aurillac.fr', 'pop': 25411},
        {'commune': 'Saint-Flour', 'dept': '15', 'url': 'https://www.saint-flour.fr', 'pop': 6643},
        {'commune': 'Arpajon-sur-CÃ¨re', 'dept': '15', 'url': 'https://www.arpajon-sur-cere.fr', 'pop': 6291},
        
        # Haute-Loire (43)
        {'commune': 'Le Puy-en-Velay', 'dept': '43', 'url': 'https://www.lepuyenvelay.fr', 'pop': 18618},
        {'commune': 'Monistrol-sur-Loire', 'dept': '43', 'url': 'https://www.monistrolsurloire.fr', 'pop': 9694},
        {'commune': 'Yssingeaux', 'dept': '43', 'url': 'https://www.yssingeaux.fr', 'pop': 7206},
        
        # RhÃ´ne (69)
        {'commune': 'Lyon', 'dept': '69', 'url': 'https://www.lyon.fr', 'pop': 522969},
        {'commune': 'Villeurbanne', 'dept': '69', 'url': 'https://www.villeurbanne.fr', 'pop': 148543},
        {'commune': 'VÃ©nissieux', 'dept': '69', 'url': 'https://www.venissieux.fr', 'pop': 64506},
        {'commune': 'Caluire-et-Cuire', 'dept': '69', 'url': 'https://www.caluire-et-cuire.fr', 'pop': 42729},
        {'commune': 'Bron', 'dept': '69', 'url': 'https://www.ville-bron.fr', 'pop': 40547},
        
        # IsÃ¨re (38)
        {'commune': 'Grenoble', 'dept': '38', 'url': 'https://www.grenoble.fr', 'pop': 158552},
        {'commune': 'Saint-Martin-d\'HÃ¨res', 'dept': '38', 'url': 'https://www.saintmartindheres.fr', 'pop': 37307},
        {'commune': 'Ã‰chirolles', 'dept': '38', 'url': 'https://www.echirolles.fr', 'pop': 35770},
        {'commune': 'Vienne', 'dept': '38', 'url': 'https://www.vienne-isere.fr', 'pop': 29400},
        {'commune': 'Fontaine', 'dept': '38', 'url': 'https://www.fontaine-isere.fr', 'pop': 21352},
    ],
    
    # === RSS FEEDS ===
    'rss_feeds': [
        # Flux municipaux
        {'nom': 'Clermont-Ferrand ActualitÃ©s', 'url': 'https://www.clermontferrand.fr/rss.xml'},
        {'nom': 'Lyon ActualitÃ©s', 'url': 'https://www.lyon.fr/rss'},
        {'nom': 'Grenoble Info', 'url': 'https://www.grenoble.fr/rss'},
        {'nom': 'Aurillac News', 'url': 'https://www.aurillac.fr/feed/'},
        {'nom': 'Le Puy RSS', 'url': 'https://www.lepuyenvelay.fr/feed/'},
        
        # Flux rÃ©gionaux
        {'nom': 'RÃ©gion AURA', 'url': 'https://www.auvergnerhonealpes.fr/actualites/rss'},
        {'nom': 'ADEME AURA', 'url': 'https://www.ademe.fr/auvergne-rhone-alpes/actualites/rss'},
    ],
    
    # === MARCHÃ‰S PUBLICS ===
    'marches_publics': [
        {'nom': 'MarchÃ©s publics AURA', 'url': 'https://www.marches-publics.gouv.fr/app.php/consultation/search?lot-dc=3&loc%5B%5D=84'},
        {'nom': 'e-marchespublics AURA', 'url': 'https://www.e-marchespublics.com/region/auvergne-rhone-alpes'},
        {'nom': 'BOAMP Ã‰nergie', 'url': 'https://www.boamp.fr/pages/recherche/?typeAO=2&motsCles=chaufferie'},
    ],
    
    # === PORTAILS SPÃ‰CIALISÃ‰S ===
    'portails_energie': [
        {'nom': 'AURA-EE (Ã‰nergie Environnement)', 'url': 'https://www.aura-ee.fr/actualites'},
        {'nom': 'RhÃ´nalpÃ©nergie', 'url': 'http://www.rhonalpenergie.fr/actualites'},
        {'nom': 'Observatoire Ã‰nergie AURA', 'url': 'https://www.auvergnerhonealpes.fr/politiques-publiques/environnement-energie'},
    ],
    
    # === INTERCOMMUNALITÃ‰S ===
    'intercommunalites': [
        {'nom': 'Clermont Auvergne MÃ©tropole', 'url': 'https://www.clermontauvergne.fr'},
        {'nom': 'Grand Lyon', 'url': 'https://www.grandlyon.com'},
        {'nom': 'Grenoble-Alpes MÃ©tropole', 'url': 'https://www.grenoblealpesmetropole.fr'},
        {'nom': 'Saint-Ã‰tienne MÃ©tropole', 'url': 'https://www.saint-etienne-metropole.fr'},
    ]
}

class ScraperNiveauPro:
    """Scraper de niveau professionnel - Extraction massive"""
    
    def __init__(self):
        self.session = requests.Session()
        self.total_sites = 0
        self.sites_ok = 0
        self.opportunites = []
        
        # Configuration session avancÃ©e
        self.session.max_redirects = 3
        
        # Pool de proxies (si nÃ©cessaire)
        self.proxies = []  # Ã€ remplir si blocages IP

    def get_headers_furtifs(self) -> dict:
        """Headers ultra-furtifs pour Ã©viter toute dÃ©tection"""
        return {
            'User-Agent': random.choice(USER_AGENTS_PRO),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Referer': random.choice([
                'https://www.google.fr/',
                'https://www.bing.com/',
                'https://duckduckgo.com/'
            ])
        }

    def analyser_texte_pro(self, texte: str, titre: str = '') -> tuple[List[str], str, Optional[str]]:
        """Analyse professionnelle avec extraction budget"""
        if not texte:
            return [], 'faible', None
            
        texte_complet = f"{titre} {texte}".lower()
        mots_trouves = []
        
        # Recherche tous mots-clÃ©s
        tous_mots_cles = MOTS_CLES_PRIORITAIRES + MOTS_CLES_SECONDAIRES
        
        for mot in tous_mots_cles:
            if mot.lower() in texte_complet:
                mots_trouves.append(mot)
        
        # DÃ©doublonnage
        mots_uniques = list(set(mots_trouves))
        
        # Calcul confiance avancÃ©
        score_prioritaire = sum(1 for mot in mots_uniques if mot in MOTS_CLES_PRIORITAIRES)
        score_total = len(mots_uniques)
        
        if score_prioritaire >= 2:
            confiance = 'forte'
        elif score_prioritaire >= 1 or score_total >= 3:
            confiance = 'moyenne'
        elif score_total >= 1:
            confiance = 'faible'
        else:
            confiance = 'nulle'
        
        # Extraction budget (regex avancÃ©es)
        budget_estime = None
        patterns_budget = [
            r'(\d{1,3}(?:[\s,.]\d{3})*)\s*â‚¬',
            r'budget\s*:?\s*(\d+(?:\s*\d+)*)\s*(?:euros?|â‚¬)',
            r'montant\s*:?\s*(\d+(?:\s*\d+)*)\s*(?:euros?|â‚¬)',
            r'prix\s*:?\s*(\d+(?:\s*\d+)*)\s*(?:euros?|â‚¬)'
        ]
        
        for pattern in patterns_budget:
            match = re.search(pattern, texte_complet, re.IGNORECASE)
            if match:
                budget_estime = match.group(1)
                break
        
        return mots_uniques, confiance, budget_estime

    def extraire_contacts(self, soup: BeautifulSoup) -> Optional[str]:
        """Extraction contacts/emails pour prospection"""
        contacts = []
        
        # Recherche emails
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, soup.get_text())
        contacts.extend(emails[:2])  # Max 2 emails
        
        # Recherche tÃ©lÃ©phones
        tel_pattern = r'(?:0[1-9](?:[\s.-]?\d{2}){4})'
        tels = re.findall(tel_pattern, soup.get_text())
        contacts.extend(tels[:1])  # Max 1 tel
        
        return ', '.join(contacts) if contacts else None

    def scraper_site_approfondi(self, site: dict, categorie: str) -> List[Opportunite]:
        """Scraping approfondi d'un site avec exploration multi-niveaux"""
        
        nom = site.get('commune', site.get('nom', 'Inconnu'))
        url = site['url']
        dept = site.get('dept', 'XX')
        pop = site.get('pop', 0)
        
        print(f"  ğŸ” {nom}")
        
        self.total_sites += 1
        opportunites_site = []
        
        try:
            # RequÃªte principale avec headers furtifs
            headers = self.get_headers_furtifs()
            response = self.session.get(url, headers=headers, timeout=20, allow_redirects=True)
            
            status = response.status_code
            print(f"    ğŸ“Š Status: {status}")
            
            if status != 200:
                if status == 403:
                    print("    ğŸš« BloquÃ© - Tentative contournement...")
                    # Tentative avec User-Agent mobile
                    headers['User-Agent'] = 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15'
                    response = self.session.get(url, headers=headers, timeout=15)
                    if response.status_code != 200:
                        return []
                else:
                    return []
            
            self.sites_ok += 1
            
            # Parse contenu principal
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraction contacts
            contacts = self.extraire_contacts(soup)
            
            # 1. ANALYSE PAGE PRINCIPALE
            texte_principal = soup.get_text()
            titre_principal = soup.find('title').get_text() if soup.find('title') else ''
            
            mots_cles, confiance, budget = self.analyser_texte_pro(texte_principal, titre_principal)
            
            if mots_cles and confiance != 'nulle':
                opportunites_site.append(Opportunite(
                    commune=nom,
                    departement=dept,
                    region='Auvergne-RhÃ´ne-Alpes',
                    source=f'{categorie}_principal',
                    date=datetime.now().strftime('%Y-%m-%d'),
                    titre=f"Page principale {nom}",
                    description=texte_principal[:400],
                    mots_cles=mots_cles,
                    url_source=url,
                    confiance=confiance,
                    population=pop,
                    budget_estime=budget,
                    contact=contacts
                ))
                print(f"    âœ… Principal: {', '.join(mots_cles[:3])} ({confiance})")
            
            # 2. EXPLORATION LIENS INTERNES (niveau 2)
            liens_interessants = []
            
            # SÃ©lecteurs avancÃ©s pour liens pertinents
            selectors_liens = [
                'a[href*="actualit"]', 'a[href*="info"]', 'a[href*="news"]',
                'a[href*="deliberation"]', 'a[href*="conseil"]', 'a[href*="municipal"]',
                'a[href*="marche"]', 'a[href*="appel"]', 'a[href*="offre"]',
                'a[href*="energie"]', 'a[href*="environnement"]', 'a[href*="travaux"]',
                'a[href*="projet"]', 'a[href*="amenagement"]'
            ]
            
            for selector in selectors_liens:
                for lien in soup.select(selector):
                    href = lien.get('href', '')
                    text = lien.get_text(strip=True)
                    
                    if href and text:
                        # Construire URL absolue
                        if href.startswith('/'):
                            href = urljoin(url, href)
                        elif not href.startswith('http'):
                            continue
                            
                        liens_interessants.append({
                            'text': text,
                            'url': href
                        })
            
            # DÃ©doublonnage liens
            liens_uniques = []
            urls_vues = set()
            for lien in liens_interessants:
                if lien['url'] not in urls_vues:
                    urls_vues.add(lien['url'])
                    liens_uniques.append(lien)
            
            print(f"    ğŸ“‹ {len(liens_uniques)} liens uniques Ã  explorer")
            
            # 3. EXPLORATION PROFONDE (max 5 liens par site)
            for lien in liens_uniques[:5]:
                try:
                    time.sleep(random.uniform(1, 2))  # Pause alÃ©atoire
                    
                    link_headers = self.get_headers_furtifs()
                    link_response = self.session.get(lien['url'], headers=link_headers, timeout=12)
                    
                    if link_response.status_code == 200:
                        link_soup = BeautifulSoup(link_response.content, 'html.parser')
                        link_texte = link_soup.get_text()
                        link_titre = link_soup.find('title').get_text() if link_soup.find('title') else lien['text']
                        
                        # Analyse contenu lien
                        mots_cles, confiance, budget = self.analyser_texte_pro(link_texte, link_titre)
                        
                        if mots_cles and confiance != 'nulle':
                            link_contacts = self.extraire_contacts(link_soup)
                            
                            opportunites_site.append(Opportunite(
                                commune=nom,
                                departement=dept,
                                region='Auvergne-RhÃ´ne-Alpes',
                                source=f'{categorie}_page',
                                date=datetime.now().strftime('%Y-%m-%d'),
                                titre=lien['text'][:120],
                                description=link_texte[:400],
                                mots_cles=mots_cles,
                                url_source=lien['url'],
                                confiance=confiance,
                                population=pop,
                                budget_estime=budget,
                                contact=link_contacts
                            ))
                            print(f"    âœ… Lien: {lien['text'][:40]}... ({confiance})")
                
                except Exception as e:
                    print(f"    âš ï¸ Erreur lien: {e}")
                    continue
        
        except Exception as e:
            print(f"    ğŸ’¥ Erreur site: {e}")
        
        return opportunites_site

    def executer_extraction_massive(self) -> List[Opportunite]:
        """Extraction massive sur toutes les sources AURA"""
        print("ğŸš€ SCRAPER NIVEAU PRO - EXTRACTION MASSIVE AURA")
        print("ğŸ’ª Mission: RÃ‰SULTATS GARANTIS pour l'entretien de Frank")
        print("ğŸ¯ Cibles: Auvergne-RhÃ´ne-Alpes (12 dÃ©partements)")
        print("=" * 80)
        
        start_time = time.time()
        
        # PHASE 1: Sites municipaux
        print("ğŸ›ï¸ PHASE 1: SITES MUNICIPAUX AURA (30 communes majeures)")
        print("=" * 60)
        
        for site in SOURCES_AUVERGNE_RHONE_ALPES['municipaux'][:15]:  # Limite Ã  15 pour commencer
            opportunites = self.scraper_site_approfondi(site, 'municipal')
            self.opportunites.extend(opportunites)
            
            # Pause progressive (plus longue si dÃ©tections)
            pause = random.uniform(2, 4) if opportunites else random.uniform(1, 2)
            time.sleep(pause)
        
        # PHASE 2: RSS Feeds
        print(f"\nğŸ“¡ PHASE 2: FLUX RSS SPÃ‰CIALISÃ‰S")
        print("=" * 40)
        
        for rss in SOURCES_AUVERGNE_RHONE_ALPES['rss_feeds']:
            opportunites = self.scraper_site_approfondi(rss, 'rss')
            self.opportunites.extend(opportunites)
            time.sleep(random.uniform(1, 2))
        
        # PHASE 3: MarchÃ©s publics
        print(f"\nğŸ’° PHASE 3: MARCHÃ‰S PUBLICS")
        print("=" * 30)
        
        for marche in SOURCES_AUVERGNE_RHONE_ALPES['marches_publics']:
            opportunites = self.scraper_site_approfondi(marche, 'marche_public')
            self.opportunites.extend(opportunites)
            time.sleep(random.uniform(1.5, 3))
        
        # PHASE 4: Portails Ã©nergie
        print(f"\nâš¡ PHASE 4: PORTAILS Ã‰NERGIE SPÃ‰CIALISÃ‰S")
        print("=" * 40)
        
        for portail in SOURCES_AUVERGNE_RHONE_ALPES['portails_energie']:
            opportunites = self.scraper_site_approfondi(portail, 'portail_energie')
            self.opportunites.extend(opportunites)
            time.sleep(random.uniform(2, 3))
        
        # PHASE 5: IntercommunalitÃ©s
        print(f"\nğŸ˜ï¸ PHASE 5: INTERCOMMUNALITÃ‰S")
        print("=" * 30)
        
        for interco in SOURCES_AUVERGNE_RHONE_ALPES['intercommunalites']:
            opportunites = self.scraper_site_approfondi(interco, 'intercommunalite')
            self.opportunites.extend(opportunites)
            time.sleep(random.uniform(2, 4))
        
        duree = time.time() - start_time
        print(f"\nâ±ï¸ EXTRACTION TERMINÃ‰E EN {duree/60:.1f} MINUTES")
        print(f"ğŸ“Š Sites traitÃ©s: {self.total_sites}")
        print(f"âœ… Sites accessibles: {self.sites_ok} ({self.sites_ok/max(self.total_sites,1)*100:.0f}%)")
        print(f"ğŸ¯ OPPORTUNITÃ‰S TROUVÃ‰ES: {len(self.opportunites)}")
        
        return self.opportunites

    def generer_rapport_pro(self, opportunites: List[Opportunite]) -> str:
        """Rapport professionnel dÃ©taillÃ©"""
        
        if not opportunites:
            return """âŒ AUCUNE OPPORTUNITÃ‰ DÃ‰TECTÃ‰E MALGRÃ‰ L'EXTRACTION MASSIVE
            
ğŸ”„ RECOMMANDATIONS URGENTES:
- Ã‰largir encore les mots-clÃ©s (inclure "rÃ©novation", "efficacitÃ©")
- Tester d'autres User-Agents ou proxies
- Explorer archives/PDF des sites
- Utiliser des APIs payantes spÃ©cialisÃ©es
            
âš ï¸ ATTENTION: Ã‰chec critique pour l'entretien de Frank"""
        
        # Statistiques avancÃ©es
        stats_source = {}
        stats_confiance = {'forte': 0, 'moyenne': 0, 'faible': 0}
        stats_dept = {}
        budget_total = 0
        contacts_total = 0
        
        for opp in opportunites:
            stats_source[opp.source] = stats_source.get(opp.source, 0) + 1
            stats_confiance[opp.confiance] += 1
            stats_dept[opp.departement] = stats_dept.get(opp.departement, 0) + 1
            
            if opp.budget_estime:
                try:
                    budget_num = int(re.sub(r'[^\d]', '', opp.budget_estime))
                    budget_total += budget_num
                except:
                    pass
                    
            if opp.contact:
                contacts_total += 1
        
        rapport = []
        rapport.append("ğŸ¯ RAPPORT PRO - EXTRACTION MASSIVE AURA")
        rapport.append("=" * 60)
        rapport.append(f"ğŸ† MISSION ACCOMPLIE POUR FRANK:")
        rapport.append(f"  â€¢ ğŸ¯ OpportunitÃ©s dÃ©tectÃ©es: {len(opportunites)}")
        rapport.append(f"  â€¢ ğŸ“Š Sources explorÃ©es: {dict(stats_source)}")
        rapport.append(f"  â€¢ ğŸ–ï¸ Confiance: Forte={stats_confiance['forte']}, Moyenne={stats_confiance['moyenne']}")
        rapport.append(f"  â€¢ ğŸ—ºï¸ DÃ©partements: {dict(stats_dept)}")
        if budget_total > 0:
            rapport.append(f"  â€¢ ğŸ’° Budget total estimÃ©: {budget_total:,}â‚¬")
        if contacts_total > 0:
            rapport.append(f"  â€¢ ğŸ“§ Contacts extraits: {contacts_total}")
        rapport.append("")
        
        # TOP OPPORTUNITÃ‰S PAR CONFIANCE
        for niveau in ['forte', 'moyenne']:
            opps_niveau = [o for o in opportunites if o.confiance == niveau]
            if opps_niveau:
                rapport.append(f"ğŸ”¥ TOP OPPORTUNITÃ‰S - CONFIANCE {niveau.upper()}")
                rapport.append("=" * 50)
                
                for i, opp in enumerate(opps_niveau[:10], 1):
                    rapport.append(f"{i}. ğŸ“ {opp.commune} ({opp.departement})")
                    rapport.append(f"   ğŸ“… {opp.date} | ğŸ”— {opp.source}")
                    if opp.population:
                        rapport.append(f"   ğŸ‘¥ Population: {opp.population:,} hab.")
                    rapport.append(f"   ğŸ“° {opp.titre}")
                    rapport.append(f"   ğŸ¯ Mots-clÃ©s: {', '.join(opp.mots_cles)}")
                    if opp.budget_estime:
                        rapport.append(f"   ğŸ’° Budget estimÃ©: {opp.budget_estime}â‚¬")
                    if opp.contact:
                        rapport.append(f"   ğŸ“§ Contact: {opp.contact}")
                    rapport.append(f"   ğŸŒ {opp.url_source}")
                    rapport.append("")
        
        # SYNTHÃˆSE POUR ENTRETIEN
        rapport.append("ğŸ’¼ SYNTHÃˆSE ENTRETIEN FRANK")
        rapport.append("=" * 35)
        
        if len(opportunites) >= 10:
            rapport.append("ğŸ† EXCELLENT RÃ‰SULTAT - Mission accomplie!")
            rapport.append("ğŸ“ˆ Preuve complÃ¨te de l'efficacitÃ© du systÃ¨me")
            rapport.append("ğŸ’ª Tu peux prÃ©senter en toute confiance")
            rapport.append("ğŸ¯ Dataset riche pour dÃ©monstration")
        elif len(opportunites) >= 5:
            rapport.append("âœ… BON RÃ‰SULTAT - Objectif atteint")
            rapport.append("ğŸ“Š Suffisant pour valider l'approche")  
            rapport.append("ğŸ”§ Quelques ajustements Ã  mentionner")
        elif len(opportunites) >= 2:
            rapport.append("âš ï¸ RÃ‰SULTAT PARTIEL - Ã€ amÃ©liorer")
            rapport.append("ğŸ”„ PrÃ©senter comme POC Ã  dÃ©velopper")
            rapport.append("ğŸ’¡ Mettre l'accent sur la technique")
        else:
            rapport.append("ğŸš¨ RÃ‰SULTAT INSUFFISANT - PROBLÃˆME")
            rapport.append("âŒ Difficile Ã  prÃ©senter en l'Ã©tat")
            rapport.append("ğŸ†˜ Besoin d'une autre stratÃ©gie")
        
        return "\n".join(rapport)

def main():
    """Fonction principale - Mission critique pour Frank"""
    print("ğŸ¯ MISSION CRITIQUE: SAUVER LA CARRIÃˆRE DE FRANK")
    print("ğŸ’ª Extraction professionnelle Auvergne-RhÃ´ne-Alpes")
    print("ğŸš€ Lancement imminent...")
    
    scraper = ScraperNiveauPro()
    
    # EXECUTION MASSIVE
    opportunites = scraper.executer_extraction_massive()
    
    # RAPPORT PROFESSIONNEL
    rapport = scraper.generer_rapport_pro(opportunites)
    
    print("\n" + "=" * 90)
    print("ğŸ“‹ RAPPORT FINAL POUR FRANK")
    print("=" * 90)
    print(rapport)
    
    # SAUVEGARDE MULTIPLE
    if opportunites:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        
        # JSON dÃ©taillÃ©
        filename_json = f'opportunites_aura_pro_{timestamp}.json'
        with open(filename_json, 'w', encoding='utf-8') as f:
            json.dump([asdict(opp) for opp in opportunites], f, ensure_ascii=False, indent=2)
        
        # CSV pour analyse
        filename_csv = f'opportunites_aura_pro_{timestamp}.csv'
        with open(filename_csv, 'w', encoding='utf-8') as f:
            f.write("Commune,Departement,Source,Date,Titre,Description,Mots_cles,URL,Confiance,Population,Budget,Contact\n")
            for opp in opportunites:
                f.write(f'"{opp.commune}","{opp.departement}","{opp.source}","{opp.date}","{opp.titre}","{opp.description}","{"; ".join(opp.mots_cles)}","{opp.url_source}","{opp.confiance}","{opp.population or ""}","{opp.budget_estime or ""}","{opp.contact or ""}"\n')
        
        print(f"\nğŸ’¾ FICHIERS GÃ‰NÃ‰RÃ‰S:")
        print(f"ğŸ“„ DonnÃ©es JSON: {filename_json}")
        print(f"ğŸ“Š Analyse CSV: {filename_csv}")
    
    print(f"\nğŸ¯ MESSAGE FINAL POUR FRANK:")
    if len(opportunites) >= 5:
        print("âœ… MISSION ACCOMPLIE! Tu as de quoi cartonner Ã  ton entretien!")
        print("ğŸ’ª Le systÃ¨me fonctionne et tu as des rÃ©sultats concrets!")
    else:
        print("âš ï¸ RÃ©sultats partiels. On va pousser encore plus loin!")
        print("ğŸš€ PrÃ©pare-toi pour la phase 2 du plan d'attaque!")

if __name__ == "__main__":
    main()